{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rolled-cotton",
   "metadata": {},
   "source": [
    "# Reweighting MC sample of $B \\to D^{*-}\\left(D^0 \\to K^{-} \\pi^+ \\pi^+ \\pi^-  \\right)X$ Background\n",
    "\n",
    "With `GBReweighter` in `hep_ml`\n",
    "\n",
    "**INPUTS**\n",
    "- `MC`: $B \\to D^{*-}\\left(D^0 \\to K^{-} \\pi^+ \\pi^+ \\pi^-  \\right)X$ \n",
    "- `data`: LHCb data, with $_s$Weights to project in the $D^+ \\to K^- \\pi^+ \\pi^-$ contribution and project out the other contributions.\n",
    "\n",
    "**GOALS**\n",
    "1. to learn the weights to apply to MC to align MC to data for the $D^0 \\to K^- \\pi^+ \\pi^+ \\pi^- $ decays by looking at the MC and $_s$Weighted LHCb data \n",
    "2. to apply the weights to the general MC.\n",
    "\n",
    "We hope that this will reweight the general very inclusive MC sample for $B \\to D^{*-} D^0 (X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daily-jason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC name:  BTODstD0X_MC\n",
      "data name:  BTODstD0X\n",
      "reweighted MC name:  GBReweighter_BTODstD0X_MC\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "from definition import (\n",
    "    columns,\n",
    "    data_names,\n",
    "    column_ranges,\n",
    ")\n",
    "\n",
    "MC_name   = data_names['MC']\n",
    "data_name = data_names['data']\n",
    "reweighted_MC_name = 'GBReweighter_' + data_names['MC']\n",
    "\n",
    "print(\"MC name: \", MC_name)\n",
    "print(\"data name: \", data_name)\n",
    "print(\"reweighted MC name: \", reweighted_MC_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "official-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/06\n"
     ]
    }
   ],
   "source": [
    "# python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zfit\n",
    "import timeit\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "\n",
    "# hep_ml\n",
    "from hep_ml.reweight import GBReweighter\n",
    "\n",
    "# bd2dsttaunu\n",
    "from bd2dsttaunu.locations import loc\n",
    "\n",
    "# HEA library\n",
    "from HEA.plot import plot_hist_auto, plot_hist, save_fig, plot_hist_var\n",
    "from HEA import load_dataframe\n",
    "from HEA.plot.tools import draw_vline\n",
    "from HEA.definition import latex_params\n",
    "from HEA.pandas_root import load_saved_root\n",
    "from HEA.pandas_root import save_root\n",
    "import HEA.BDT as bt\n",
    "from HEA.tools.serial import dump_joblib, retrieve_joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-grammar",
   "metadata": {},
   "source": [
    "## Read the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loaded-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/correiaa/bd2dsttaunu/output//root/BTODstD0X_MC/BTODstD0X_MC_with_sWeights.root\n",
      "Loading /home/correiaa/bd2dsttaunu/output//root/BTODstD0X/BTODstD0X_with_sWeights.root\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "df['MC'] = load_saved_root(MC_name + '_with_sWeights', folder_name=MC_name)\n",
    "df['data'] = load_saved_root(data_name + '_with_sWeights', folder_name=data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "precise-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data'] = df['data'].dropna()\n",
    "df['MC'] = df['MC'].dropna()\n",
    "\n",
    "df['data'].reset_index(drop=True, inplace=True)\n",
    "df['MC'].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-little",
   "metadata": {},
   "source": [
    "## Prepare the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unlimited-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 available columns:\n",
      "- m_Kpipipi\n",
      "- q2_reco\n",
      "- isolation_bdt\n",
      "- tau_life_reco\n",
      "- m_DstKpipi\n",
      "- theta_X_reco\n",
      "- theta_L_reco\n",
      "- chi_reco\n",
      "- costheta_X_reco\n",
      "- costheta_L_reco\n",
      "- coschi_reco\n",
      "- tau_M\n",
      "- B0_M\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(columns)} available columns:\")\n",
    "for column in columns:\n",
    "    print(\"- \" + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "communist-tooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 columns used for training of the GBRWeighter:\n",
      "- costheta_X_reco\n",
      "- costheta_L_reco\n",
      "- chi_reco\n",
      "- isolation_bdt\n",
      "- q2_reco\n"
     ]
    }
   ],
   "source": [
    "training_columns = [\n",
    "    'costheta_X_reco',\n",
    "    'costheta_L_reco',\n",
    "    'chi_reco',\n",
    "    'isolation_bdt',\n",
    "    'q2_reco',\n",
    "]\n",
    "\n",
    "print(f\"{len(training_columns)} columns used for training of the GBRWeighter:\")\n",
    "for training_column in training_columns:\n",
    "    print(\"- \" + training_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-northwest",
   "metadata": {},
   "source": [
    "We'll add the $_s$Weights to this list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-therapy",
   "metadata": {},
   "source": [
    "### Split the dataframes in two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "humanitarian-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC\n",
      "data\n",
      "12768 12768\n",
      "26615 26615\n"
     ]
    }
   ],
   "source": [
    "two_dfs = {\n",
    "    1 : {},\n",
    "    2 : {}\n",
    "}\n",
    "\n",
    "for type_df in df.keys():\n",
    "    print(type_df)\n",
    "    two_dfs[1][type_df] = df[type_df].sample(frac=0.5, random_state=20)\n",
    "    two_dfs[2][type_df] = df[type_df].drop(two_dfs[1][type_df].sort_index().index,0).sample(frac=1., random_state=20)\n",
    "\n",
    "# check\n",
    "for type_df in df.keys():\n",
    "    print(len(two_dfs[1][type_df]),len(two_dfs[2][type_df]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-vegetation",
   "metadata": {},
   "source": [
    "### Get the dataframes with only the training variables + the $_s$Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "divided-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_two_dfs = {\n",
    "    1 : {},\n",
    "    2 : {}\n",
    "}\n",
    "for num in reweight_two_dfs.keys():\n",
    "    reweight_two_dfs[num]['MC'] = two_dfs[num]['MC'][training_columns]\n",
    "    reweight_two_dfs[num]['data'] = two_dfs[num]['data'][training_columns + ['sWeight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "running-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in two_dfs.keys():\n",
    "    for type_df in two_dfs[num].keys():\n",
    "        assert (reweight_two_dfs[num]['MC'].columns == training_columns).all()\n",
    "        assert (reweight_two_dfs[num]['data'].columns == training_columns + ['sWeight']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "surprised-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Divide into training and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "directed-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reweight_two_dfs = {\n",
    "    1 : {},\n",
    "    2 : {}\n",
    "}\n",
    "test_reweight_two_dfs = {\n",
    "    1 : {},\n",
    "    2 : {}\n",
    "}\n",
    "\n",
    "for num in two_dfs.keys():\n",
    "    for type_df in two_dfs[num].keys():\n",
    "        train_reweight_two_dfs[num][type_df], test_reweight_two_dfs[num][type_df] =\\\n",
    "            train_test_split(reweight_two_dfs[num][type_df], \n",
    "                             test_size=0.5,\n",
    "                             random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "curious-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6384 6384\n",
      "13307 13308\n",
      "6384 6384\n",
      "13307 13308\n"
     ]
    }
   ],
   "source": [
    "for type_df in two_dfs.keys():\n",
    "    for num in two_dfs[type_df].keys():\n",
    "        print(len(train_reweight_two_dfs[type_df][num]), len(test_reweight_two_dfs[type_df][num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-sunrise",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "strange-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HEA.tools import get_chi2_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "significant-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(train_weights, test_weights):\n",
    "    return ks_2samp(train_weights, test_weights).statistic\n",
    "\n",
    "def compute_pvalue(train_weights, test_weights):\n",
    "    return ks_2samp(train_weights, test_weights).pvalue\n",
    "\n",
    "def compute_chi2(data_test_df, MC_test_df, data_test_weight, MC_test_weight, n_bins=15):\n",
    "    columns = data_test_df.columns\n",
    "    chi2 = 0\n",
    "    for column in columns:\n",
    "        data = data_test_df[column]\n",
    "        original_MC = MC_test_df[column]\n",
    "                \n",
    "        chi2 += get_chi2_2samp(original_MC, data, \n",
    "                               n_bins=n_bins,\n",
    "                               low=None, high=None,\n",
    "                               weights1=MC_test_weight, \n",
    "                               weights2=data_test_weight)\n",
    "    return chi2\n",
    "\n",
    "def get_best_params(param_grid,\n",
    "                    original_train_df, target_train_df, \n",
    "                    original_test_df, target_test_df,\n",
    "                    target_train_weight=None, \n",
    "                    target_test_weight=None,\n",
    "                    **fit_params):\n",
    "    best_param_values = {}\n",
    "\n",
    "    param_values_lists = product(*param_grid.values())\n",
    "    param_names = param_grid.keys()\n",
    "    \n",
    "    best_score = np.inf\n",
    "    \n",
    "    scores = {}\n",
    "    p_values = {}\n",
    "    \n",
    "    for param_values_list in param_values_lists:\n",
    "        \n",
    "        hyperparams = dict(zip(param_names, param_values_list))\n",
    "        print(\"test of \")\n",
    "        print(hyperparams)\n",
    "        \n",
    "        reweighter = GBReweighter(gb_args={'random_state': 20},\n",
    "                                  **hyperparams)\n",
    "        reweighter = reweighter.fit(\n",
    "            original=original_train_df,\n",
    "            target=target_train_df,\n",
    "            target_weight=target_train_weight,\n",
    "            **fit_params\n",
    "        )\n",
    "        \n",
    "        MC_test_weight = reweighter.predict_weights(original_test_df)\n",
    "        MC_train_weight = reweighter.predict_weights(original_train_df)\n",
    "        \n",
    "        scores[tuple(param_values_list)] = compute_chi2(\n",
    "            data_test_df=target_test_df,\n",
    "            MC_test_df=original_test_df,\n",
    "            data_test_weight=target_test_weight,\n",
    "            MC_test_weight=MC_test_weight,\n",
    "        )\n",
    "        \n",
    "        p_values[tuple(param_values_list)] = compute_pvalue(MC_train_weight, MC_test_weight)\n",
    "        \n",
    "        \n",
    "        if scores[tuple(param_values_list)] < best_score:\n",
    "            best_score = scores[tuple(param_values_list)]\n",
    "            best_param_values = tuple(param_values_list)\n",
    "        \n",
    "    return scores, p_values, best_param_values  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(10, 66, 5),\n",
    "    'learning_rate': [0.08],\n",
    "    \"max_depth\": [2]\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "best_param_values = {}\n",
    "p_values = {}\n",
    "for num in train_reweight_two_dfs.keys():\n",
    "    scores[num], p_values[num], best_param_values[num] = get_best_params(\n",
    "        param_grid,\n",
    "        original_train_df=train_reweight_two_dfs[num]['MC'], \n",
    "        target_train_df=train_reweight_two_dfs[num]['data'].drop('sWeight', 1),\n",
    "        original_test_df=test_reweight_two_dfs[num]['MC'],\n",
    "        target_test_df=test_reweight_two_dfs[num]['data'].drop('sWeight', 1),\n",
    "        target_train_weight=train_reweight_two_dfs[num]['data']['sWeight'],\n",
    "        target_test_weight=test_reweight_two_dfs[num]['data']['sWeight'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd2dsttaunu_env",
   "language": "python",
   "name": "bd2dsttaunu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
